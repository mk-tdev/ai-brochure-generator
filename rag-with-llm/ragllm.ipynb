{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c304c585",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import litellm\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from chromadb import PersistentClient\n",
    "from tqdm import tqdm\n",
    "from litellm import completion\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from ollama import Client\n",
    "from utils import fetch_documents\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "# litellm.enable_json_schema_validation = True\n",
    "\n",
    "MODEL=\"ollama/gpt-oss:120b\"\n",
    "api_base=\"https://ollama.com\"\n",
    "extra_headers={\n",
    "    \"Authorization\": os.environ.get('OLLAMA_API_KEY')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b848d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by LangChain's Document - let's have something similar\n",
    "\n",
    "class Result(BaseModel):\n",
    "    page_content: str\n",
    "    metadata: dict\n",
    "\n",
    "# A class to perfectly represent a chunk\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    headline: str = Field(description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\")\n",
    "    summary: str = Field(description=\"A few sentences summarizing the content of this chunk to answer common questions\")\n",
    "    original_text: str = Field(description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\")\n",
    "\n",
    "    def as_result(self, document):\n",
    "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
    "        return Result(page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text,metadata=metadata)\n",
    "\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    chunks: list[Chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52880b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = fetch_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(document):\n",
    "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
    "    return f\"\"\"\n",
    "    You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
    "\n",
    "    The document is from the shared drive of a company called Insurellm.\n",
    "    The document is of type: {document[\"type\"]}\n",
    "    The document has been retrieved from: {document[\"source\"]}\n",
    "\n",
    "    A chatbot will use these chunks to answer questions about the company.\n",
    "    You should divide up the document as you see fit, being sure that the entire document is returned in the chunks - don't leave anything out.\n",
    "    This document should probably be split into {how_many} chunks, but you can have more or less as appropriate.\n",
    "    There should be overlap between the chunks as appropriate; typically about 25% overlap or about 50 words, so you have the same text in multiple chunks for best retrieval results.\n",
    "\n",
    "    For each chunk, you should provide a headline, a summary, and the original text of the chunk.\n",
    "    Together your chunks should represent the entire document with overlap.\n",
    "\n",
    "    Here is the document:\n",
    "\n",
    "    {document[\"text\"]}\n",
    "\n",
    "    Respond with the chunks.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_prompt(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_messages(document):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65312484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_messages(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    messages = make_messages(document)\n",
    "    # response = completion(model=MODEL, messages=messages, api_base=api_base, extra_headers=extra_headers, response_format=Chunks)\n",
    "    # response = completion(model='ollama/deepseek-v3.1:671b-cloud', messages=messages, api_base=api_base, extra_headers=extra_headers, response_format=Chunks, format=Chunks.model_json_schema())\n",
    "    response = completion(model='gpt-4.1-nano', messages=messages, response_format=Chunks)\n",
    "    reply = response.choices[0].message.content\n",
    "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "    return [chunk.as_result(document) for chunk in doc_as_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_document(documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
